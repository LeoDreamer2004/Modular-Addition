\documentclass{article}

\usepackage[final]{neurips_2022}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsmath}        % math
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{xeCJK}
\usepackage{microtype}
\usepackage{xcolor}         % colors
\usepackage{graphicx}       % images
\usepackage{subcaption}
\usepackage{algorithm, algorithmicx, algpseudocode}

\setCJKmainfont{SimSun}
\setCJKmonofont{SimSun}
\title{Final Project Report}

\author{
    陈润璘 \\
    \texttt{2200010848} \\
    \And
    任子博 \\
    \texttt{2200010626} \\
    \And
    原梓轩 \\
    \texttt{2200010825} \\
}

\begin{document}

\maketitle

\begin{abstract}
    This report is based on the work presented in \textit{Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets} \cite{power2022grokking}, which introduces the phenomenon wherein neural networks achieve improved generalization performance long after overfitting to the training data. In this study, we reproduce the grokking phenomenon across various model architectures, train-test splits, optimizers, and regularization techniques. Additionally, we propose potential explanations for this behavior based on experimental results. Our code is open-source and available at GitHub repository: \url{https://github.com/LeoDreamer2004/Modular-Addition}.
\end{abstract}

\section{Introduction}

Classical learning theory generally predicts that overparameterized models should overfit to the training data, leading to poor generalization performance. Contrary to this expectation, modern neural networks frequently exhibit exceptional generalization capabilities. The paper \textit{Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets} introduces the concept of "grokking," where neural networks demonstrate a sudden and substantial improvement in generalization performance long after they have overfitted to the training data.

In this report, we aim to replicate the primary findings of the original study, focusing on the grokking phenomenon and its influencing factors. Specifically, we train various neural networks to learn the modular addition task $(x, y) \rightarrow (x + y)\, {\rm mod}\, p$, where $p$ is a prime number. Through these experiments, we explore the impact of model architecture, train-test data split, optimizer selection, and regularization techniques on the grokking phenomenon, and provide potential explanations for the observed results..

\section{Methodology}

We select the prime number $p = 97$ to generate a dataset comprising $97^2$ samples, each consisting of two integers $x$ and $y$ in the range $[0, 96]$. The target output for each sample is the result of $(x + y) \mod 97$. By decomposing the equation $x + y = z$ into individual tokens, the task is reformulated as a next-token prediction problem. Inputs are encoded using one-hot representations of the tokens.

For this study, we train Transformer, Multilayer Perceptron (MLP), and Long Short-Term Memory (LSTM) models using a $50\%$ train-test split. While hyperparameters were kept consistent across experiments, slight adjustments were made to the learning rate in the $60\%$ training data case to stabilize training; we believe this adjustment does not affect the conclusions.

Furthermore, we investigate the effects of different optimizers, regularization techniques, and batch sizes on the grokking phenomenon. For weight decay and dropout experiments, all other hyperparameters remain unchanged. Batch sizes of 256, 512, and full-batch training are also evaluated to examine their impact on model performance and generalization.

\section{Experiments and Results}

\subsection{Grokking on Transformer Model}
The Transformer model, equipped with an attention mechanism, is widely used for natural language processing (NLP) tasks due to its powerful representation capabilities. In our experiments, we trained a Transformer with a single layer, embedding size of 512, 4 attention heads, and a feedforward dimension of 256. The model was optimized using the AdamW optimizer with a weight decay of $10^{-4}$ over a budget of $10^5$ epochs. Training was conducted with train-test splits of $60\%$, $50\%$, $45\%$, and $40\%$. The training and testing accuracy curves for these splits are presented in Figure \ref{fig:transformer-accuracy}, with more detailed results available in Figure \ref{fig:transformer-accuracy-all} in the appendix.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\textwidth]{../code/result/transformer/accuracy-split.png}
    \caption{Accuracy of the Transformer model with different train-test splits.}
    \label{fig:transformer-accuracy}
\end{figure}

The results indicate that the grokking phenomenon is observed with training data proportions of $60\%$, $50\%$, and $45\%$. As the proportion of training data decreases, the model requires more epochs to generalize effectively. For the $40\%$ training data case, while the model successfully memorizes the training set, it fails to generalize to unseen data.

To analyze the grokking phenomenon in greater detail, we examined the training and testing loss for the Transformer model with $50\%$ and $45\%$ training data splits, as shown in Figure \ref{fig:transformer-loss}. During the memorization phase, the training loss decreases rapidly, but the testing loss initially stagnates or slightly increases. After a large number of epochs, the testing loss begins to decline sharply, signifying the onset of grokking.

\begin{figure}[ht]
    \centering
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\textwidth]{../code/fig/transformer/loss-adamw-0.5-2.png}
        \caption{Train percentage: 50\%}
    \end{subfigure}
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\textwidth]{../code/fig/transformer/loss-adamw-0.55-2.png}
        \caption{Train percentage: 45\%}
    \end{subfigure}
    \caption{Loss of Transformer model with 50\% and 45\% train percentage}
    \label{fig:transformer-loss}
\end{figure}

\subsection{Grokking on MLP and LSTM Model}

Unlike the Transformer model, the Multilayer Perceptron (MLP) lacks self-attention and the ability to capture token memory. In contrast, the Long Short-Term Memory (LSTM) network, a type of recurrent neural network, excels in modeling long-term dependencies. We trained both MLP and LSTM models on the same dataset to compare their grokking phenomena with the Transformer model.

The MLP architecture consisted of four layers: input $\rightarrow$ 256 $\rightarrow$ 128 $\rightarrow$ 128 $\rightarrow$ output, with ReLU activation functions applied between layers and layer normalization included to enhance generalization. As shown in Figure \ref{fig:mlp-accuracy}, the MLP model initially achieves high accuracy on the training dataset while the test accuracy remains below $10\%$. However, as the cross-entropy loss decreases, the test accuracy gradually improves, eventually reaching nearly $100\%$, demonstrating the grokking phenomenon.

\begin{figure}[ht]
    \centering
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\textwidth]{../code/fig/mlp/accuracy-adam-0.5.png}
        \caption{MLP}
        \label{fig:mlp-accuracy}
    \end{subfigure}
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\textwidth]{../code/fig/lstm/accuracy-adam-0.5.png}
        \caption{LSTM}
        \label{fig:lstm-accuracy}
    \end{subfigure}
    \caption{Accuracy of MLP and LSTM models with a $50\%$ train-test split.}
    \label{fig:mlp-lstm-accuracy}
\end{figure}

The LSTM model was designed with two layers, each comprising 128 hidden units, followed by a layer normalization layer to improve generalization. It was trained using the Adam optimizer. Similar to the MLP and Transformer models, the LSTM also exhibited the grokking phenomenon, as shown in Figure \ref{fig:lstm-accuracy}. The test accuracy remains low during the initial training phase but begins to increase significantly after several epochs, eventually achieving high generalization performance.

These results confirm that the grokking phenomenon is not exclusive to a particular architecture. However, the rate and extent of grokking vary across models, reflecting differences in their ability to capture and generalize from the training data.

\subsection{Impact of Optimizers, Regularization Techniques and Batch Size}

In this section, we analyze the effects of optimizers, regularization techniques, and batch size on the grokking phenomenon. The experiments were conducted using the same Transformer model on a $50\%$ train-test split due to computational constraints. The model was trained using SGD, SignGD, and RMSProp optimizers to assess the role of optimization methods. Regularization techniques, including dropout and varying weight decay values, were also applied. Finally, we examined the influence of batch size by training the model with different configurations.

\begin{figure}[ht]
    \centering
    \begin{subfigure}{0.3\textwidth}
        \includegraphics[width=\textwidth]{../code/fig/transformer/accuracy-sgd-0.5-2.png}
        \caption{SGD}
    \end{subfigure}
    \begin{subfigure}{0.3\textwidth}
        \includegraphics[width=\textwidth]{../code/fig/transformer/accuracy-signgd-0.5.png}
        \caption{SignGD}
    \end{subfigure}
    \begin{subfigure}{0.3\textwidth}
        \includegraphics[width=\textwidth]{../code/fig/transformer/accuracy-rmsprop-0.5-2.png}
        \caption{RMSProp}
    \end{subfigure}
    \caption{Accuracy of Transformer model with different optimizers}
    \label{fig:transformer-optimizer}
\end{figure}

Figure \ref{fig:transformer-optimizer} illustrates the effects of various optimizers on grokking. While the choice of optimizer alters convergence speed and the shape of training curves, the grokking phenomenon consistently occurs in all cases, suggesting that it is not strongly dependent on the specific optimization method.

\begin{figure}[htbp]
    \centering
    \begin{minipage}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../code/result/transformer/accuracy-regularization.png}
        \caption{Accuracy of Transformer model with different regularization techniques}
        \label{fig:transformer-regularization}
    \end{minipage}
    \hspace{0.05\textwidth}
    \begin{minipage}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../code/result/transformer/accuracy-batchsize.png}
        \caption{Accuracy of Transformer model with different batch size}
        \label{fig:transformer-batchsize}
    \end{minipage}
\end{figure}

The influence of regularization techniques is presented in Figure \ref{fig:transformer-regularization}. Both weight decay and dropout enhance the model's generalization capabilities, reducing the number of epochs required for grokking to occur. This indicates that regularization helps mitigate overfitting, allowing the model to achieve generalization more efficiently.

In Figure \ref{fig:transformer-batchsize}, we examined the impact of batch size on the grokking phenomenon. Smaller batch sizes introduce stochasticity into the training process, which appears to accelerate generalization, reducing the epochs needed for grokking. In contrast, training with a full batch size fails to achieve generalization within the $10^4$ epochs budget, likely due to the lack of noise, which hinders exploration of the loss landscape.

\subsection{Grokking Phenomena on Harder Problems}

In this section, we trained a Transformer model to solve a more complex task: learning modular addition $(x, y, z) \rightarrow (x + y + z)\, {\rm mod}\, p$, where $p$ is a prime number. To manage the increased dataset size and problem complexity, we selected a smaller prime, $p=23$, and increased the number of attention heads to 8 to enhance the model's capacity.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.45\textwidth]{../code/fig/transformer/accuracy-adamw-0.6-3.png}
    \caption{Accuracy of Transformer model with $40\%$ train-test split on the harder problem.}
    \label{fig:transformer-harder}
\end{figure}

As shown in Figure \ref{fig:transformer-harder}, the grokking phenomenon persists but is less pronounced compared to the simpler problem. This suggests that solving more complex tasks increases the difficulty of achieving grokking. In many cases, the model either fails to generalize or exhibits a gradual increase in test accuracy that parallels the improvement in training accuracy, rather than the sharp transition characteristic of grokking.

These findings highlight the sensitivity of grokking to task complexity and hyperparameter selection. While the phenomenon remains observable, achieving it for harder problems demands a deeper exploration of the training process and adjustments to model configurations.

\section{Explanation of Grokking Phenomena}

DeMoss et al.(2024) give a theoretical explanation of the grokking phenomenon based on Kolmogorov complexity and rate-distortion theory in \cite{demoss2024complexity}. We reproduce the experiments and results in the paper and provide a detailed explanation of the grokking phenomenon.

\subsection{Kolmogorov Complexity and Rate-Distortion Theory}

It is widely acknowledged that the complexity of a model plays a crucial role in determining its generalization performance. To formalize this relationship, we introduce Kolmogorov complexity $K(h)$, as defined in algorithmic information theory (AIT). Kolmogorov complexity represents the minimum number of bits required to encode a hypothesis or model $h$. Lofti et al. provided an upper bound on the expected risk $R(h)$ in terms of $K(h)$:
$$
R(h) \leq \hat{R}(h) + O\left(\sqrt{\frac{K(h)}{n}}\right),
$$
where $\hat{R}(h)$ denotes the empirical risk, and $n$ is the number of samples.

Inspired by information theory, we also consider the algorithmic rate-distortion function $r_x(y)$, which measures the minimum complexity required to encode a hypothesis $y$ under a distortion constraint:
$$
r_x(y) = \min_y\left\{ K(y): d(x, y) \leq \epsilon \right\},
$$
where $d$ is the distortion function, and $\epsilon$ is the maximum allowable distortion. In this context, the distortion function quantifies the difference between the true risk and its approximation. The rate-distortion function $r_x(y)$ is a non-increasing function of distortion, meaning that higher allowable distortion results in lower complexity.

To ensure model stability, we define the distortion function using the loss function $\ell$:
$$
d(\theta, \hat{\theta}) = \left| \ell(\theta, D) - \ell(\hat{\theta}, D) \right|,
$$
where $\theta$ represents the model parameters and $D$ the dataset. By controlling the trade-off between rate and distortion, we can manage the balance between model complexity and generalization. Specifically, $\epsilon$ serves as a criterion for accepting approximations, ensuring that the deviation in loss is bounded.

\subsection{Compression Techniques}

Although Kolmogorov complexity is theoretically uncomputable, practical upper bounds can be estimated using standard compression algorithms. In our work, we use \texttt{bzip2} as a representative example to approximate the complexity of a model or data.

Principal Component Analysis (PCA) is another widely used compression technique in machine learning. Given a rank $r$, PCA provides a low-rank approximation of the original data matrix $X$ by minimizing the Frobenius norm of the difference between the original matrix and its approximation. The objective function for PCA is defined as:
$$
\hat{X} = \arg\min_{\hat{X}} \left\| X - \hat{X} \right\|_F^2 \quad \text{s.t.} \quad \text{rank}(\hat{X}) \le r.
$$
This ensures that the compressed representation retains the most significant variance in the data, reducing redundancy while preserving essential features.

To further facilitate discretization of the model parameters into forms compatible with computational processing, we introduce a quantization scheme with a bin spacing $\Delta$. The quantization function $Q$ is defined as:
$$
Q(\theta) = \Delta \left\lfloor \frac{\theta}{\Delta} \right\rfloor,
$$
where $\theta$ represents the model parameters. Quantization helps reduce the precision of the parameters, which can also indirectly lower the model complexity and storage requirements.

To find the optimal hyperparameters for compression, we employ Bayesian Optimization tools. The complexity of the model is evaluated as the compressed file size produced by \texttt{bzip2}, while ensuring stability in the model's performance. Algorithm \ref{alg:compression} outlines the model compression process using Bayesian Optimization.

\begin{algorithm}[!ht]
    \renewcommand{\algorithmicrequire}{\textbf{Input:}}
    \renewcommand{\algorithmicensure}{\textbf{Output:}}
    \caption{Model Compression with Bayesian Optimization}
    \label{alg:compression}
    \begin{algorithmic}[1]
        \Require Model parameters $\theta$, optimizer steps $N$, error tolerance $\epsilon$;
        \Ensure Compressed model parameters $\hat{\theta}$;
        \State Initialize Bayesian Optimization $B$ with $\theta$ and output of rank $r_i$, quantization space $\Delta_i$;
        \State Best parameters $\hat{\theta}$ and complexity $\hat{s} \leftarrow \infty$;

        \For {$i=1,2,\cdots,N$}
        \State $r_i$, $\Delta_i$ $\leftarrow$ $B$.optimize();
        \Comment Suggestions from Bayesian Optimizer
        \State $\theta \leftarrow R(\theta$, $r_i$);
        \Comment Principal Component Analysis
        \State $\theta \leftarrow Q(\theta$, $\Delta_i$);
        \Comment Quantization
        \If {$| \ell(\theta, D) - \ell(\hat{\theta}, D) | \le \epsilon$}
        \Comment Accept the approximation
        \State $s \leftarrow$ \texttt{bzip2}($\theta$);
        \Comment Naive compression to estimate the complexity
        \State Update $\hat{s}$ and $\hat{\theta}$ with $s$ and $\theta$ if needed;
        \EndIf
        \EndFor
        \State \Return $\hat{\theta}$;
    \end{algorithmic}
\end{algorithm}

\subsection{Phenomena and Explanation}

Our compression experiments provide a dynamic measure of model complexity during training. As illustrated in Figure \ref{fig:complexity}, the training process can be divided into two distinct phases:

\begin{itemize}
    \item \textbf{Memorizing}: During the initial phase, the model focuses on memorizing the training data. This results in a steady increase in model complexity, as captured by the compression size. However, the model demonstrates poor performance on the test dataset, indicating overfitting to the training data.
    \item \textbf{Generalizing}: In the later phase, overfitting leads the model to identify low-rank representations that capture the underlying structure of the data. This results in a sharp decrease in model complexity as redundancy is minimized. Despite the reduced complexity, the model retains high accuracy on the training data, indicating that it has learned key data features. At this point, the model transitions from overfitting to effective generalization, which aligns with the grokking phenomenon.
\end{itemize}

This experiment reveals the fundamental cause of the grokking phenomenon: model complexity. Due to the small size of the dataset, the model easily overfits the data while attempting to analyze its principal features. As training progresses, the model's Kolmogorov complexity decreases, aligning more closely with the data and thereby improving its generalization performance.

Our results support the hypothesis that grokking emerges as a consequence of this interplay between overfitting and complexity reduction. In essence, the phenomenon is driven by the model's ability to shift from a memorization-focused phase to one where it extracts essential features, thereby achieving a balance between complexity and generalization.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.45\textwidth]{../code/result/complexity_new.png}
    \caption{Model complexity alongside the training}
    \label{fig:complexity}
\end{figure}

\newpage

\appendix

\section{Appendix}

Figure \ref{fig:transformer-accuracy-all} gives more details about the accuracy of Transformer model with different train percentage.

\begin{figure}[ht]
    \centering
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\textwidth]{../code/fig/transformer/accuracy-adamw-0.5-2.png}
        \caption{Train percentage: 50\%}
    \end{subfigure}
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\textwidth]{../code/fig/transformer/accuracy-adamw-0.4-2.png}
        \caption{Train percentage: 60\%}
    \end{subfigure}
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\textwidth]{../code/fig/transformer/accuracy-adamw-0.55-2.png}
        \caption{Train percentage: 45\%}
    \end{subfigure}
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\textwidth]{../code/fig/transformer/accuracy-adamw-0.6-2.png}
        \caption{Train percentage: 40\%}
    \end{subfigure}
    \caption{Accuracy of Transformer model with different train percentage}
    \label{fig:transformer-accuracy-all}
\end{figure}

\bibliographystyle{unsrt}
\bibliography{references}

\end{document}
