\documentclass{article}

\usepackage[final]{neurips_2022}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsmath}        % math
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}
\usepackage{xcolor}         % colors
\usepackage{graphicx}       % images
\usepackage{subcaption}
\usepackage{algorithm, algorithmicx, algpseudocode}


\title{Final Project Report}

\author{
    Author1 \\
    \texttt{email} \\
    \And
    Author2 \\
    \texttt{email} \\
    \And
    Author3 \\
    \texttt{email} \\
}

\begin{document}

\maketitle

\begin{abstract}
    This report is based on the paper \textit{Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets} \cite{power2022grokking}, which introduces phenomena that networks improve generalization performance long after overfitting. In this report, we reproduce the Grokking phenomena with different models, train-test split, optimizers and regularization techniques. We also provide explanations for the Grokking phenomena based on the experiments and results.
\end{abstract}

\section{Introduction}

Unlike classical learning theory, which often predicts that overparameterized models should overfit, modern neural networks frequently exhibit remarkable generalization abilities. The paper introduced a novel concept called "grokking," where neural networks demonstrate a sudden improvement in generalization performance long after they have already overfit the training data.

In this report, we aim to replicate the key findings of the original paper, focusing on the phenomenon of grokking and the factors that influence it. By training different neural networks to learn modular addition $(x, y) \rightarrow (x + y)\, {\rm mod}\, p$ where $p$ is a prime number, we investigate how the choice of model architecture, train-test split, optimizer, and regularization techniques affect the grokking phenomenon and give a possible explanation for it.

\section{Methodology}

We choose the prime number $p=97$ and generate a dataset of $97^2$ samples, each consisting of two integers $x$ and $y$ in the range $[0, 96]$. The target output is the sum of $x$ and $y$ modulo $97$. By splitting the equation $x + y = z$ into tokens, we convert the problem into a predict next token task. For encoding the input, we use a one-hot encoding of the tokens.

In this project, we train Transformer, Multilayer Perceptron (MLP), and Long Short-Term Memory (LSTM) models with $50\%$ train data and $50\%$ test data. Also, we tried to train model with $60\%$, $45\%$ and $40\%$ train data with almost same hyperparameters \footnote{To make the train process more stable, we slightly change the learning rate with $60\%$ train data, but we think it does not effect the conclusion. For other cases, all hyperparameters are the same.}. Moreover, we experiment with different optimizers, regularization techniques and batch size to investigate their impact on the grokking phenomenon. When we change weight decay and dropout, all other hyperparameters are the same. For batch size, we use 256, 512 and full batch size.

\section{Experiments and Results}

\subsection{Grokking on Transformer Model}

Transformer is a powerful model contains attention mechanism, which is widely used in NLP tasks. We train a Transformer model with 1 layer, embedding size 512, 4 attention heads and 256 feedforward dimension. The model is trained with AdamW optimizer with weight decay $1e-4$ and $10^5$ epochs budget. In our experiments, we take the train data percentage as $50\%$, $60\%$, $45\%$ and $40\%$. The curve of training accuracy and test accuracy with different train percentage is shown in Figure \ref{fig:transformer-accuracy}, for more detailed results, see Figure \ref{fig:transformer-accuracy-all} in the appendix.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\textwidth]{../code/result/transformer/accuracy-split.png}
    \caption{Accuracy of Transformer model with different train percentage}
    \label{fig:transformer-accuracy}
\end{figure}

From the results, we can see that the grokking phenomenon occurs with $50\%$, $60\%$ and $45\%$ train data percentage. With less train data, it takes more epochs to generalize. For $40\%$ train data percentage although the model can memorize all training data, it fails to generalize on testing data.

Lets look into the grokking phenomenon in more detail. We plot the training and testing loss of the Transformer model with $50\%$ and $45\%$ train data percentage in Figure \ref{fig:transformer-loss}. When the model is memorizing the training data, the training loss decreases rapidly, however, the testing loss does not decrease, even increases a bit. After many epochs, the testing loss starts to decrease, which is the grokking phenomenon.

\begin{figure}[ht]
    \centering
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\textwidth]{../code/fig/transformer/loss-adamw-0.5-2.png}
        \caption{Train percentage: 50\%}
    \end{subfigure}
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\textwidth]{../code/fig/transformer/loss-adamw-0.55-2.png}
        \caption{Train percentage: 45\%}
    \end{subfigure}
    \caption{Loss of Transformer model with 50\% and 45\% train percentage}
    \label{fig:transformer-loss}
\end{figure}

\subsection{Grokking on MLP and LSTM Model}

Unlike Transformer, MLP does not have the self-attention mechanism and the memory to given tokens. That is why MLP is considered to be less powerful than Transformer in natural language processing tasks. LSTM is a type of recurrent neural network that can learn long-term dependencies. We train MLP and LSTM models on the same dataset and compare their grokking phenomena with the Transformer model.

Particularly, we design the MLP model with layers: (input $\rightarrow$ 256 $\rightarrow $128$ \rightarrow $128$ \rightarrow $output) with ReLU activation functions inserted between each layer. Also, we introduced a layer normalized layer to strengthen the model's generalization ability. Thus, we observed the grokking phenomena in the MLP model, as the Figure \ref{fig:mlp-accuracy} shows.

Apparently, the accuracy on test dataset remains less than 10\% despite the model's high accuracy on the training dataset at first. However, as the cross entropy loss continues to decrease to some extent, the accuracy on the test dataset start gradually climbing up to nearly 100\%.

At the same time, we designed a LSTM model with 2 layers, each with 128 hidden units. For better generalization, we added a layer normalization layer at the end of LSTM layer. We trained the LSTM model with Adam optimizer. The grokking phenomena in the LSTM model is shown in the Figure \ref{fig:lstm-accuracy}.

\begin{figure}[ht]
    \centering
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\textwidth]{../code/fig/mlp/accuracy-adam-0.5.png}
        \caption{MLP}
        \label{fig:mlp-accuracy}
    \end{subfigure}
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\textwidth]{../code/fig/lstm/accuracy-adam-0.5.png}
        \caption{LSTM}
        \label{fig:lstm-accuracy}
    \end{subfigure}
    \caption{Accuracy of MLP and LSTM model with 50\% train percentage}
    \label{fig:mlp-lstm-accuracy}
\end{figure}
Similar to the MLP and Transformer models, the LSTM model also exhibits the grokking phenomenon. The accuracy on the test dataset remains low at the beginning, but it starts to increase after a certain number of epochs.

\subsection{Impact of Optimizers, Regularization Techniques and Batch Size}

In this section, we investigate the impact of different optimizers and regularization techniques on the grokking phenomenon. We train the same Transformer model with SGD, SignGD and RMSProp optimizers. Also, we tried to train the model with dropout, and different weight decay to investigate the influence of regularization techniques. Finally, to investigate how batch size influence grokking phenomena, we use different batch size to train the model. Experiments are only done on $50\%$ train data percentage because of limitation of computational resources.

\begin{figure}[ht]
    \centering
    \begin{subfigure}{0.3\textwidth}
        \includegraphics[width=\textwidth]{../code/fig/transformer/accuracy-sgd-0.5-2.png}
        \caption{SGD}
    \end{subfigure}
    \begin{subfigure}{0.3\textwidth}
        \includegraphics[width=\textwidth]{../code/fig/transformer/accuracy-signgd-0.5.png}
        \caption{SignGD}
    \end{subfigure}
    \begin{subfigure}{0.3\textwidth}
        \includegraphics[width=\textwidth]{../code/fig/transformer/accuracy-rmsprop-0.5-2.png}
        \caption{RMSProp}
    \end{subfigure}
    \caption{Accuracy of Transformer model with different optimizers}
    \label{fig:transformer-optimizer}
\end{figure}

Figure \ref{fig:transformer-optimizer} shows how different optimizers affect the grokking phenomenon. Although the difference between optimizers leads to different convergence speeds and curve shapes, the grokking phenomenon still occurs in all cases.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\textwidth]{../code/result/transformer/accuracy-regularization.png}
    \caption{Accuracy of Transformer model with different regularization techniques}
    \label{fig:transformer-regularization}
\end{figure}

Figure \ref{fig:transformer-regularization} shows the impact of different regularization techniques on the grokking phenomenon. We can see that both  weight decay and dropout can help the model generalize better, which means it takes less epochs to generalize beyond overfitting.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\textwidth]{../code/result/transformer/accuracy-batchsize.png}
    \caption{Accuracy of Transformer model with different batch size}
    \label{fig:transformer-batchsize}
\end{figure}

Figure \ref{fig:transformer-batchsize} shows the impact of different batch size on the grokking phenomenon. As we can see, although smaller batch size leads to noise in the training curve, it takes less epochs to achieve generalization. The model trained with full batch fails to generalize with $10^4$ epochs budget.

\subsection{Grokking Phenomena on Harder Problems}

In this section, we trained a Transformer model to learn the modular addition $(x, y, z) \rightarrow (x + y + z)\, {\rm mod}\, p$ where $p$ is a prime number. Due to the increased data size and complexity of the problem, we choose a smaller prime number $p=23$. Also, to fit the harder problem, we increase the number of attention heads to 8. Figure \ref{fig:transformer-harder} shows the accuracy of the Transformer model with $40\%$ train percentage on the harder problem.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\textwidth]{../code/fig/transformer/accuracy-adamw-0.6-3.png}
    \caption{Accuracy of Transformer model with 40\% train percentage on harder problem}
    \label{fig:transformer-harder}
\end{figure}

Although there is still grokking phenomenon, it is less usual to happen compared to the easier problem, which means we should tune the hyperparameters more carefully to achieve grokking on harder problems. Based on our model, more common cases are either the model fails to generalize or the test accuracy increases along with the training accuracy.

\section{Explanation of Grokking Phenomena}

\subsection{Kolmogorov Complexity and Rate-Distortion Theory}

It is universally considered that the complexity of the model determined the generalization performance. We introduces Kolmogorov complexity $K(h)$ in algorithmic information theory (AIT), which is the minimum number of bits required to describe the string to encode. Lofti et al. shew a upper bound of expected risk $R(h)$ with the given complexity:
$$
R(h) \leq \hat{R}(h) + O\left(\sqrt{\frac{K(h)}{n}}\right)
$$
where $\hat{R}(h)$ is the empirical risk of the model, $n$ is the number of samples. 

Just as what information theory does, algorithmic rate-distortion function $r_x(y)$ is defined as the minimum mutual information (here the complexity) when the distortion (here the risk) is below a certain threshold. 
$$
r_x(y) = \min_y\left\{ K(y): d(x, y) \le \epsilon \right\}
$$
where $d$ is the distortion function. The rate-distortion function is a non-increasing function of the distortion. In order to keep the stability of the model, we choose the distortion function under the loss function $d(\theta, \hat{\theta}) = | \ell(\theta, D) - \ell(\hat{\theta}, D) |$. And thus, we can control the rate-distortion trade-off by adjusting $\epsilon$, which serves as a criterion to accept the approximation:
$$
\left| \ell(\theta, D) - \ell(\hat{\theta}, D) \right| \le \epsilon
$$

\subsection{Compression Techniques}

While Kolmogorov complexity is uncomputable, its upper bound can be estimated with easy compression methods, and we take \texttt{bzip2} as an instance. 

Principal Component Analysis (PCA) is a common compression technique in machine learning. With the given rank $r$, PCA can be seen as a low rank approximation of the original data matrix $X$. PCA acquires the optimal low rank approximation by minimizing the Frobenius norm of the difference between the original data matrix and the compressed data matrix. The compression function is defined as:
$$
\hat{X} = \arg\min_{\hat{X}} \left\| X - \hat{X} \right\|_F^2 \quad \text{s.t.} \quad \text{rank}(\hat{X}) \le r
$$

Meanwhile, in order to discretize the data into computer-friendly forms, we plot a quantization on the model parameters with a bin spacing $\Delta$, and therefore introduce a quantization function $Q$:
$$
Q(\theta) = \Delta \left\lfloor \frac{\theta}{\Delta} \right\rfloor
$$

We apply Bayesian Optimaztion tools to find the optimal hyperparameters for the model. We simply evaluate the complexity of the model with the bzip size after compression, with the condition of stability.


\begin{algorithm}[!ht]
    \renewcommand{\algorithmicrequire}{\textbf{Input:}}
    \renewcommand{\algorithmicensure}{\textbf{Output:}}
    \caption{Model Compression with Bayesian Optimization}
    \label{ADMM}
    \begin{algorithmic}[1]
        \Require Model parameters $\theta$, optimizer steps $N$, error tolerance $\epsilon$;
        \Ensure Compressed model parameters $\hat{\theta}$;
        \State Initialize Bayesian Optimization $B$ with the given hyperparameters;
        \State Best hyperparameters $\hat{\theta}$ and size $s \leftarrow \infty$;
    
        % % if ... else
        % \If {$a < 0$}
        %     \State $y = 2;$
        % \Else
        %     \State $y = 4;$
        % \EndIf
    
        % while
        % \While {$a > 0$}
            % \State $a--;$
        % \EndWhile
    
        % for loop
        \For {$i=1,2,\cdots,N$}
        \State rank $r_i$, quantization space $\Delta_i$ $\leftarrow$ $B$.optimize();
        \Comment Suggestions from Bayesian Optimizer

        \EndFor
  
        % return
        \State \Return $c$.
 \end{algorithmic}
\end{algorithm}


\appendix

\section{Appendix}

Figure \ref{fig:transformer-accuracy-all} shows the accuracy of Transformer model with different train percentage in more detail.

\begin{figure}[ht]
    \centering
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\textwidth]{../code/fig/transformer/accuracy-adamw-0.5-2.png}
        \caption{Train percentage: 50\%}
    \end{subfigure}
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\textwidth]{../code/fig/transformer/accuracy-adamw-0.4-2.png}
        \caption{Train percentage: 60\%}
    \end{subfigure}
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\textwidth]{../code/fig/transformer/accuracy-adamw-0.55-2.png}
        \caption{Train percentage: 45\%}
    \end{subfigure}
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\textwidth]{../code/fig/transformer/accuracy-adamw-0.6-2.png}
        \caption{Train percentage: 40\%}
    \end{subfigure}
    \caption{Accuracy of Transformer model with different train percentage}
    \label{fig:transformer-accuracy-all}
\end{figure}

\bibliographystyle{plain}
\bibliography{references}

\end{document}
