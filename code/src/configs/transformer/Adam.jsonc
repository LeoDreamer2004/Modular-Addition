{
  "model_path": "../model/transformer.pth",
  "optimizer": "Adam",
  "scheduler": "lambda",
  "lambda_lr_func": "transformer_adam",
  "scheduler_step_size": 100,
  "scheduler_gamma": 0.9,
  "modulus": 97,
  "epoch_num": 5000,
  "lr": 0.0003,
  "seed": 0,
  "batch_size": 512,
  "test_alpha": 0.5,
  "d_model": 512,
  "n_head": 4,
  "dim_feedforward": 256,
  "n_layers": 1,
  "max_seq_length": 6,
  "dropout": 0,
  "log_interval": 10,
  "save_interval": 1000,
  // Gradient clipping
  "max_grad_norm": 1
}